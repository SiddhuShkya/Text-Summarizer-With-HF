{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf23559",
   "metadata": {},
   "source": [
    "GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1706428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1706428",
    "outputId": "10539b6e-3c8b-49cd-ebb0-5adcd7231596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  1 09:27:43 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   32C    P8              3W /   50W |     472MiB /   4096MiB |     25%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3986      G   /usr/lib/xorg/Xorg                       39MiB |\n",
      "|    0   N/A  N/A           26118      C   python                                  424MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de3294",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6877ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d6877ae",
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "outputId": "49972d84-1963-4257-b5a7-e2c57a0b698b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/siddhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6ab47",
   "metadata": {},
   "source": [
    "Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-NOO-pYPO5u5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NOO-pYPO5u5",
    "outputId": "71f5c67d-e2f5-4d24-e404-b5d98e14553a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device :  cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # Choose GPU if available, else CPU\n",
    "print(\"Device : \", device)  # Print the selected device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53905aae",
   "metadata": {},
   "source": [
    "Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e0d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"  # Specify the pre-trained model name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # Load tokenizer for the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(\n",
    "    device\n",
    ")  # Load model and move to device\n",
    "print(f\"Model loaded successfully on {device}\")  # Confirm model is loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d4b1a",
   "metadata": {},
   "source": [
    "Test Model on Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5111863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock markets saw a significant dip on Monday as investors reacted to new inflation data.\n"
     ]
    }
   ],
   "source": [
    "article = \"\"\"\n",
    "The stock market saw a significant dip on Monday as investors reacted to new inflation data. \n",
    "Tech stocks were the hardest hit, with major indices dropping by over 2%. Analysts suggest \n",
    "that while the volatility is concerning, long-term projections remain stable if interest \n",
    "rates hold steady.\n",
    "\"\"\"  # Sample article to summarize\n",
    "\n",
    "input_text = \"summarize: \" + article  # Add prefix to indicate summarization task\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\n",
    "    device\n",
    ")  # Tokenize and move to device\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,  # Maximum length of generated summary\n",
    "    min_length=15,  # Minimum length of generated summary\n",
    "    num_beams=4,  # Beam search for better quality\n",
    "    no_repeat_ngram_size=3,  # Prevent repeating trigrams\n",
    "    repetition_penalty=2.5,  # Penalize repetition\n",
    "    length_penalty=1.0,  # Control length preference\n",
    "    early_stopping=True,  # Stop when all beams finish\n",
    ")\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    ")  # Decode and print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb533e",
   "metadata": {},
   "source": [
    "Download & Extract Dataset (Data Ingestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f474e146",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f474e146",
    "outputId": "8f416d5c-ee18-4a2e-ec58-69b44e6f3f44"
   },
   "outputs": [],
   "source": [
    "def download_and_extract(url: str, zip_name: str = \"summarizer-data.zip\", extract_dir: str = \"summarizer-data\"):\n",
    "    \n",
    "    # Download\n",
    "    if not os.path.exists(zip_name):\n",
    "        print(\"Downloading dataset...\")\n",
    "        urllib.request.urlretrieve(url, zip_name)\n",
    "    else:\n",
    "        print(\"Zip file already exists. Skipping download.\")\n",
    "    \n",
    "    # Extract\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "\n",
    "    with zipfile.ZipFile(zip_name, \"r\") as zip_ref:\n",
    "        # Get top-level folder(s) in zip\n",
    "        top_level_dirs = set(f.split(\"/\")[0] for f in zip_ref.namelist() if f.strip())\n",
    "        # Check if top-level folder(s) exist already\n",
    "        already_extracted = all(os.path.exists(os.path.join(extract_dir, d)) for d in top_level_dirs)\n",
    "        if already_extracted:\n",
    "            print(\"Dataset already extracted. Skipping unzip.\")\n",
    "        else:\n",
    "            print(\"Extracting dataset...\")\n",
    "            # Flatten: remove the top-level folder from the zip paths\n",
    "            for f in zip_ref.namelist():\n",
    "                if f.strip():  # skip empty entries\n",
    "                    path_parts = f.split(\"/\")\n",
    "                    # Skip the first folder in path\n",
    "                    target_path = os.path.join(extract_dir, *path_parts[1:])\n",
    "                    if f.endswith(\"/\"):\n",
    "                        os.makedirs(target_path, exist_ok=True)\n",
    "                    else:\n",
    "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
    "                        with open(target_path, \"wb\") as out_file:\n",
    "                            out_file.write(zip_ref.read(f))\n",
    "            print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a6dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Extracting dataset...\n",
      "Extraction complete.\n"
     ]
    }
   ],
   "source": [
    "download_and_extract(\n",
    "    url=\"https://github.com/SiddhuShkya/Text-Summarizer-With-HF/raw/main/data/summarizer-data.zip\"\n",
    ")  # Download and unzip dataset from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15250b1",
   "metadata": {},
   "source": [
    "Load Data as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a3c059",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47a3c059",
    "outputId": "9837e786-a4bd-4e8b-ee70-60739a03f315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:  ['id', 'dialogue', 'summary']\n",
      "======================================================================\n",
      "Number of samples in each dataset:\n",
      "Train data samples:  14731\n",
      "Test data samples:  819\n",
      "Validation data samples:  818\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./summarizer-data/train.csv\")  # Load training dataset\n",
    "test_df = pd.read_csv(\"./summarizer-data/test.csv\")  # Load test dataset\n",
    "val_df = pd.read_csv(\"./summarizer-data/validation.csv\")  # Load validation dataset\n",
    "\n",
    "print(\n",
    "    \"Features in the dataset: \", train_df.columns.tolist()\n",
    ")  # Print dataset column names\n",
    "print(\"=\" * 70)\n",
    "print(\"Number of samples in each dataset:\")\n",
    "print(\"Train data samples: \", len(train_df))  # Print number of training samples\n",
    "print(\"Test data samples: \", len(test_df))  # Print number of test samples\n",
    "print(\"Validation data samples: \", len(val_df))  # Print number of validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b901ccd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b901ccd",
    "outputId": "6e5c1aa8-2a09-41a5-c60a-ff0aa097a66c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Summary:  Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"dialogue\"][0])  # Print first dialogue/sample from training data\n",
    "print(\n",
    "    \"\\nSummary: \", train_df[\"summary\"][0]\n",
    ")  # Print corresponding summary of the first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8afb9",
   "metadata": {},
   "source": [
    "Tokenize & Prepare Features (Data Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70fe037c",
   "metadata": {
    "id": "70fe037c"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    model_inputs = tokenizer(\n",
    "        example_batch[\"dialogue\"],  # Tokenize input dialogue\n",
    "        text_target=example_batch[\"summary\"],  # Tokenize target summary for seq2seq\n",
    "        max_length=1024,  # Max length for input tokens\n",
    "        truncation=True,  # Truncate if longer than max_length\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=example_batch[\"summary\"],\n",
    "        max_length=128,\n",
    "        truncation=True,  # Tokenize summary as labels\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # Add tokenized labels to model inputs\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057387aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "4c0abfc4d95f4064b310c02ca81cd816",
      "e66d9e7fb3ec4828bb249d4cf96a5cac",
      "3534ed580a8b4d558dcea8cb6bac2b90",
      "4589041c0a514c809b499c7589ff910e",
      "ee9c860558aa468b8256631a5356415d",
      "07c0f7d2cd324ab0a2d0914140ef027b",
      "e6fdb18a828d445a9895d5d7b1ec0422",
      "6fe4a53246094fc79d3dffc3ef79185d",
      "7ac91ebbdf54487ea306b9038e8241db",
      "aa2fd1988f6c452eb9f4891e302cb2ba",
      "866ae5e771d94255a445e7bce03c8774",
      "1d86684a798b4203875635ac87b3e135",
      "d8f4f80c387c4adcb0edafeddfec8318",
      "210a04495dc344aba864d9d6d29bb8a1",
      "513b5c0b7057438c957344cddadc2564",
      "b36f54fb686f4dc8be98508c65369b56",
      "44608b02839842f4a133941520b38d10",
      "447503cfffba42468e6ae9425106c101",
      "08e33a8a118c45f2a30bb68037fa5e8d",
      "b8199cbac42248e6988dcd640e4f24f0",
      "230e5f42e70a4c0986d3eae032eb3c0f",
      "9d031235979142f1937ca37e36c96fd3",
      "d1b4c42fc04148e9b43802b30ec92fcd",
      "bbd7b7a0ee8c45bab75abb8c5c15bb5c",
      "33d57cd3608a4778a5153b4f0116e74d",
      "9c20fd432f624628ba169de475095151",
      "dff510b3f1824ad0865e716e96409c1a",
      "b2c44efd43c24971b46681877a234661",
      "9ff5fb79d2b94248a1b196ca3eff8e49",
      "af3b0e7a510c4728aa85f76eac7cd5f0",
      "d6d4820fc63f4cb79d197ae4eb14f6bf",
      "8f9d17fa93524923af2ddaf7948e9b61",
      "99a2eaff194f46198471269f0fa1575c"
     ]
    },
    "id": "057387aa",
    "outputId": "29f4ed5c-1315-4e37-b38f-83f58ecd4ddc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ce420c0b3d4d2890d5941a6a9d8020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bece84ccf6f40088c149263a8a4ae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90f868ea03f45c4bdada2d8e8bd633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert dataframes â†’ Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "# Apply the function with .map()\n",
    "train_dataset = train_dataset.map(convert_examples_to_features, batched=True)\n",
    "test_dataset = test_dataset.map(convert_examples_to_features, batched=True)\n",
    "val_dataset = val_dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "TEgN8895ZCkc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEgN8895ZCkc",
    "outputId": "cc54bc5e-31c0-4ac0-cf2f-1d144bafcae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      " Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 14731\n",
      "})\n",
      "Test Dataset:\n",
      " Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 819\n",
      "})\n",
      "Val Dataset:\n",
      " Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 818\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset:\\n\", train_dataset)  # Display tokenized training dataset\n",
    "print(\"Test Dataset:\\n\", test_dataset)  # Display tokenized test dataset\n",
    "print(\"Val Dataset:\\n\", val_dataset)  # Display tokenized validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbec24a",
   "metadata": {},
   "source": [
    "Set Up Training & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e049d56",
   "metadata": {
    "id": "5e049d56"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,  # Tokenizer to dynamically pad inputs\n",
    "    model=model,  # Model to handle special tokens for seq2seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "675931bb",
   "metadata": {
    "id": "675931bb"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"flan-t5-small-finetuned\",  # Directory to save model checkpoints\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    warmup_steps=500,  # Steps for learning rate warmup\n",
    "    per_device_train_batch_size=1,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=1,  # Batch size per device during evaluation\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    logging_steps=10,  # Log training metrics every 10 steps\n",
    "    eval_strategy=\"steps\",  # Evaluate model at specified steps\n",
    "    eval_steps=500,  # Evaluation interval in steps\n",
    "    save_steps=100000,  # Save checkpoint every 100k steps\n",
    "    gradient_accumulation_steps=16,  # Accumulate gradients over multiple steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2000b919",
   "metadata": {
    "id": "2000b919"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,  # Model to train\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=test_dataset,  # Dataset used for training (here using test dataset)\n",
    "    eval_dataset=val_dataset,  # Dataset used for evaluation\n",
    "    data_collator=data_collator,  # Collate batches dynamically\n",
    "    processing_class=tokenizer,  # Tokenizer for preprocessing inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19447fe5",
   "metadata": {},
   "source": [
    "Finetuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29aa25a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "29aa25a9",
    "outputId": "18bb500b-20f0-48c7-f4d4-9c0c5521d45e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/52 00:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=52, training_loss=2.004348736542922, metrics={'train_runtime': 52.0986, 'train_samples_per_second': 15.72, 'train_steps_per_second': 0.998, 'total_flos': 45322428678144.0, 'train_loss': 2.004348736542922, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # Start the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de0234",
   "metadata": {},
   "source": [
    "Test the fine-tuned Model (Generate Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a26630cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to evaluation mode\n",
    "model.eval()               # Set model to evaluation mode (disable dropout, etc.)\n",
    "model.to(device)           # Move model to the selected device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c344b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The dog was so lazy that it didn't even bark at the fox.\n"
     ]
    }
   ],
   "source": [
    "# Test text \n",
    "test_text = \"\"\"summarize: The quick brown fox jumps over the lazy dog. \n",
    "            The dog was so lazy that it didn't even bark at the fox. \n",
    "            This event was caught on camera by a local hiker.\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    truncation=True,  # Truncate if text exceeds max_length\n",
    "    padding=\"max_length\",  # Pad to max_length\n",
    "    max_length=512,  # Maximum input length\n",
    ").to(device)\n",
    "\n",
    "# Generate Summary\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],  # Input token IDs\n",
    "        max_length=50,  # Maximum summary length\n",
    "        num_beams=4,  # Beam search for better quality\n",
    "        early_stopping=True,  # Stop generation when beam search is complete\n",
    "    )\n",
    "\n",
    "print(\"Summary:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c36743",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e58cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Split a list into smaller batches of given size\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]  # Yield each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ixjL5yhfeY2B",
   "metadata": {
    "id": "ixjL5yhfeY2B"
   },
   "outputs": [],
   "source": [
    "def calculate_metric_on_test_ds(\n",
    "    dataset,\n",
    "    metric,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    column_text=\"article\",\n",
    "    column_summary=\"highlights\",\n",
    "):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Split dataset into batches\n",
    "    article_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_text], batch_size)\n",
    "    )\n",
    "    target_batches = list(\n",
    "        generate_batch_sized_chunks(dataset[column_summary], batch_size)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for article_batch, target_batch in zip(article_batches, target_batches):\n",
    "            inputs = tokenizer(\n",
    "                article_batch,\n",
    "                max_length=256,  # Max input length\n",
    "                truncation=True,  # Truncate if too long\n",
    "                padding=\"max_length\",  # Pad to max length\n",
    "                return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "            )\n",
    "\n",
    "            summaries = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"].to(device),  # Move inputs to device\n",
    "                attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                max_new_tokens=128,  # Max tokens for output\n",
    "                num_beams=1,  # Beam search width\n",
    "                do_sample=False,  # Deterministic generation\n",
    "                use_cache=True,  # Use past key values for speed\n",
    "            )\n",
    "\n",
    "            decoded_summaries = tokenizer.batch_decode(\n",
    "                summaries,\n",
    "                skip_special_tokens=True,  # Decode outputs to text\n",
    "            )\n",
    "\n",
    "            metric.add_batch(\n",
    "                predictions=decoded_summaries,  # Add generated summaries\n",
    "                references=target_batch,  # Add reference summaries\n",
    "            )\n",
    "\n",
    "    return metric.compute()  # Compute final metric (e.g., ROUGE, BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ZcrxIRCgvbe",
   "metadata": {
    "id": "2ZcrxIRCgvbe"
   },
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")  # Load ROUGE evaluation metric\n",
    "rouge_names = [\n",
    "    \"rouge1\",\n",
    "    \"rouge2\",\n",
    "    \"rougeL\",\n",
    "    \"rougeLsum\",\n",
    "]  # Specify ROUGE variants to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Jl0tbhT3hPFD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "Jl0tbhT3hPFD",
    "outputId": "f491391e-e675-4855-dad3-2343fdc7b716"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>flan-t5-small-finetuned</th>\n",
       "      <td>0.364824</td>\n",
       "      <td>0.116339</td>\n",
       "      <td>0.286061</td>\n",
       "      <td>0.286229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           rouge1    rouge2    rougeL  rougeLsum\n",
       "flan-t5-small-finetuned  0.364824  0.116339  0.286061   0.286229"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = calculate_metric_on_test_ds(\n",
    "    dataset=test_dataset[0:50],  # Evaluate on first 50 samples of test dataset\n",
    "    metric=rouge_metric,  # Use the loaded ROUGE metric\n",
    "    model=trainer.model,  # Model to generate summaries\n",
    "    tokenizer=tokenizer,  # Tokenizer for preprocessing\n",
    "    column_text=\"dialogue\",  # Column containing input text\n",
    "    column_summary=\"summary\",  # Column containing reference summaries\n",
    ")\n",
    "\n",
    "# Extract only the specified ROUGE scores\n",
    "rouge_dict = {name: score[name] for name in rouge_names}\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "pd.DataFrame(rouge_dict, index=[\"flan-t5-small-finetuned\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cadefe",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "TrN7rtC6X2uO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrN7rtC6X2uO",
    "outputId": "7546011f-c8e3-4f1f-b629-cce6ade3feb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-tokenizer/tokenizer_config.json',\n",
       " 't5-tokenizer/special_tokens_map.json',\n",
       " 't5-tokenizer/spiece.model',\n",
       " 't5-tokenizer/added_tokens.json',\n",
       " 't5-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"t5-model\")  # Save the fine-tuned model to disk\n",
    "tokenizer.save_pretrained(\"t5-tokenizer\")  # Save the tokenizer to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc46a66",
   "metadata": {},
   "source": [
    "Testing the saved model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "NLIzvDpCfYQ3",
   "metadata": {
    "id": "NLIzvDpCfYQ3"
   },
   "outputs": [],
   "source": [
    "model_path = \"./t5-model\"  # Path to the saved fine-tuned model\n",
    "tokenizer_path = \"./t5-tokenizer\"  # Path to the saved tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)  # Load the saved tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)  # Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9jOWPJp8fcCM",
   "metadata": {
    "id": "9jOWPJp8fcCM"
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_length\": 20,  # Maximum length of generated text\n",
    "    \"min_length\": 5,  # Minimum length of generated text\n",
    "    \"length_penalty\": 2.0,  # Penalize shorter sequences\n",
    "    \"num_beams\": 4,  # Beam search width for better quality\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "uLH8HHTOfoGS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uLH8HHTOfoGS",
    "outputId": "d4264ac2-9cc9-4fe1-c0b0-11b3141a7e52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "sample_text = train_dataset[0][\"dialogue\"]  # Take first dialogue from training dataset\n",
    "reference = train_dataset[0][\"summary\"]  # Take corresponding reference summary\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"summarization\",  # Use pipeline for text summarization\n",
    "    model=model,  # Use the loaded/fine-tuned model\n",
    "    tokenizer=tokenizer,  # Use the corresponding tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "H5L5itfzfqFi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5L5itfzfqFi",
    "outputId": "dbb6af82-1dd4-453d-9a41-e2ba1c4e5fbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue : \n",
      " Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Reference Summary : \n",
      " Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "Model Summary : \n",
      " Amanda baked cookies. Jerry will bring Amanda tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogue : \\n\", sample_text)  # Print the input dialogue\n",
    "print(\"\\nReference Summary : \\n\", reference)  # Print the reference summary\n",
    "print(\n",
    "    \"\\nModel Summary : \\n\", pipe(sample_text, **gen_kwargs)[0][\"summary_text\"]\n",
    ")  # Generate and print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "YhRjJqt3n_jy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhRjJqt3n_jy",
    "outputId": "02c642e5-3d6f-46eb-bd9c-9ccff1f88876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIALOGUE ---\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "--- REFERENCE SUMMARY (Ground Truth) ---\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n",
      "\n",
      "--- MODEL GENERATED SUMMARY ---\n",
      "Amanda baked cookies. Jerry will bring Amanda tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# 1. Grab your text from the dataset\n",
    "sample_text = train_dataset[0][\"dialogue\"]  # Input dialogue from dataset\n",
    "reference = train_dataset[0][\"summary\"]  # Reference summary for comparison\n",
    "\n",
    "# 2. Tokenize the input dialogue\n",
    "# truncation=True ensures it fits within the model's 1024 token limit\n",
    "inputs = tokenizer(\n",
    "    sample_text, truncation=True, padding=\"longest\", return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# 3. Generate the summary\n",
    "# Model outputs token IDs for the summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=128,  # Maximum tokens for generated summary\n",
    "    num_beams=4,  # Beam search width for better quality\n",
    "    length_penalty=2.0,  # Favor longer sequences\n",
    "    early_stopping=True,  # Stop generation when beams finish\n",
    ")\n",
    "\n",
    "# 4. Decode the IDs back into a string\n",
    "decoded_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 5. Compare the results\n",
    "print(\"--- DIALOGUE ---\")\n",
    "print(sample_text)  # Print original dialogue\n",
    "print(\"\\n--- REFERENCE SUMMARY (Ground Truth) ---\")\n",
    "print(reference)  # Print reference summary\n",
    "print(\"\\n--- MODEL GENERATED SUMMARY ---\")\n",
    "print(decoded_summary)  # Print model-generated summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c0f7d2cd324ab0a2d0914140ef027b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e33a8a118c45f2a30bb68037fa5e8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d86684a798b4203875635ac87b3e135": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8f4f80c387c4adcb0edafeddfec8318",
       "IPY_MODEL_210a04495dc344aba864d9d6d29bb8a1",
       "IPY_MODEL_513b5c0b7057438c957344cddadc2564"
      ],
      "layout": "IPY_MODEL_b36f54fb686f4dc8be98508c65369b56"
     }
    },
    "210a04495dc344aba864d9d6d29bb8a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e33a8a118c45f2a30bb68037fa5e8d",
      "max": 819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8199cbac42248e6988dcd640e4f24f0",
      "value": 819
     }
    },
    "230e5f42e70a4c0986d3eae032eb3c0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33d57cd3608a4778a5153b4f0116e74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af3b0e7a510c4728aa85f76eac7cd5f0",
      "max": 818,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6d4820fc63f4cb79d197ae4eb14f6bf",
      "value": 818
     }
    },
    "3534ed580a8b4d558dcea8cb6bac2b90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fe4a53246094fc79d3dffc3ef79185d",
      "max": 14731,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ac91ebbdf54487ea306b9038e8241db",
      "value": 14731
     }
    },
    "44608b02839842f4a133941520b38d10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "447503cfffba42468e6ae9425106c101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4589041c0a514c809b499c7589ff910e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa2fd1988f6c452eb9f4891e302cb2ba",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_866ae5e771d94255a445e7bce03c8774",
      "value": "â€‡14731/14731â€‡[00:19&lt;00:00,â€‡770.80â€‡examples/s]"
     }
    },
    "4c0abfc4d95f4064b310c02ca81cd816": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e66d9e7fb3ec4828bb249d4cf96a5cac",
       "IPY_MODEL_3534ed580a8b4d558dcea8cb6bac2b90",
       "IPY_MODEL_4589041c0a514c809b499c7589ff910e"
      ],
      "layout": "IPY_MODEL_ee9c860558aa468b8256631a5356415d"
     }
    },
    "513b5c0b7057438c957344cddadc2564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_230e5f42e70a4c0986d3eae032eb3c0f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9d031235979142f1937ca37e36c96fd3",
      "value": "â€‡819/819â€‡[00:00&lt;00:00,â€‡1138.14â€‡examples/s]"
     }
    },
    "6fe4a53246094fc79d3dffc3ef79185d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ac91ebbdf54487ea306b9038e8241db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "866ae5e771d94255a445e7bce03c8774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f9d17fa93524923af2ddaf7948e9b61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99a2eaff194f46198471269f0fa1575c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c20fd432f624628ba169de475095151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f9d17fa93524923af2ddaf7948e9b61",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_99a2eaff194f46198471269f0fa1575c",
      "value": "â€‡818/818â€‡[00:00&lt;00:00,â€‡1651.21â€‡examples/s]"
     }
    },
    "9d031235979142f1937ca37e36c96fd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ff5fb79d2b94248a1b196ca3eff8e49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa2fd1988f6c452eb9f4891e302cb2ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af3b0e7a510c4728aa85f76eac7cd5f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2c44efd43c24971b46681877a234661": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b36f54fb686f4dc8be98508c65369b56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8199cbac42248e6988dcd640e4f24f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bbd7b7a0ee8c45bab75abb8c5c15bb5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2c44efd43c24971b46681877a234661",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9ff5fb79d2b94248a1b196ca3eff8e49",
      "value": "Map:â€‡100%"
     }
    },
    "d1b4c42fc04148e9b43802b30ec92fcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbd7b7a0ee8c45bab75abb8c5c15bb5c",
       "IPY_MODEL_33d57cd3608a4778a5153b4f0116e74d",
       "IPY_MODEL_9c20fd432f624628ba169de475095151"
      ],
      "layout": "IPY_MODEL_dff510b3f1824ad0865e716e96409c1a"
     }
    },
    "d6d4820fc63f4cb79d197ae4eb14f6bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8f4f80c387c4adcb0edafeddfec8318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44608b02839842f4a133941520b38d10",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_447503cfffba42468e6ae9425106c101",
      "value": "Map:â€‡100%"
     }
    },
    "dff510b3f1824ad0865e716e96409c1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e66d9e7fb3ec4828bb249d4cf96a5cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07c0f7d2cd324ab0a2d0914140ef027b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e6fdb18a828d445a9895d5d7b1ec0422",
      "value": "Map:â€‡100%"
     }
    },
    "e6fdb18a828d445a9895d5d7b1ec0422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee9c860558aa468b8256631a5356415d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
